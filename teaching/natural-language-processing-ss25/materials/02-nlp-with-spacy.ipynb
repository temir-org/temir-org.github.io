{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40w-f01kwVUS"
      },
      "source": [
        "# An Introduction to Natural Language in Python using spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtVRNgRgwXMG"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This tutorial provides a brief introduction to working with natural language (sometimes called \"text analytics\") in Python, using [spaCy](https://spacy.io/) and related libraries.\n",
        "Data science teams in industry must work with lots of text, one of the top four categories of data used in machine learning.\n",
        "Usually that's human-generated text, although not always.\n",
        "\n",
        "Think about it: how does the \"operating system\" for business work? Typically, there are contracts (sales contracts, work agreements, partnerships), there are invoices, there are insurance policies, there are regulations and other laws, and so on.\n",
        "All of those are represented as text.\n",
        "\n",
        "You may run across a few acronyms: _natural language processing_ (NLP), _natural language understanding_ (NLU), _natural language generation_ (NLG) — which are roughly speaking \"read text\", \"understand meaning\", \"write text\" respectively.\n",
        "Increasingly these tasks overlap and it becomes difficult to categorize any given feature.\n",
        "\n",
        "The _spaCy_ framework — along with a wide and growing range of plug-ins and other integrations — provides features for a wide range of natural language tasks.\n",
        "It's become one of the most widely used natural language libraries in Python for industry use cases, and has quite a large community — and with that, much support for commercialization of research advances as this area continues to evolve rapidly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNd1a1eewexp"
      },
      "source": [
        "## (If you are not in online colab) Getting Started\n",
        "\n",
        "Check out the excellent _spaCy_ [installation notes](https://spacy.io/usage) for a \"configurator\" which generates installation commands based on which platforms and natural languages you need to support.\n",
        "\n",
        "Some people tend to use `pip` while others use `conda`, and there are instructions for both.  For example, to get started with _spaCy_ working with text in English and installed via `conda` on a Linux system:\n",
        "```\n",
        "conda install -c conda-forge spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "BTW, the second line above is a download for language resources (models, etc.) and the `_sm` at the end of the download's name indicates a \"small\" model. There's also \"medium\" and \"large\", albeit those are quite large. Some of the more advanced features depend on the latter, although we won't quite be diving to the bottom of that ocean in this (brief) tutorial.\n",
        "\n",
        "Now let's load _spaCy_ and run some code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDG49oQEy3sf"
      },
      "source": [
        "## (If you are in online colab) Start here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSos7OfowuB5"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUJ-Pbvnw4Dh"
      },
      "source": [
        "That nlp variable is now your gateway to all things spaCy and loaded with the en_core_web_sm small model for English. Next, let's run a small \"document\" through the natural language parser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1nxmgGUw6fX"
      },
      "outputs": [],
      "source": [
        "text = \"The weather is really nice today :)\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJowPsRNw_-R"
      },
      "source": [
        "First we created a [doc](https://spacy.io/api/doc) from the text, which is a container for a document and all of its annotations. Then we iterated through the document to see what _spaCy_ had parsed.\n",
        "\n",
        "Good, but it's a lot of info and a bit difficult to read. Let's reformat the _spaCy_ parse of that sentence as a [pandas](https://pandas.pydata.org/) dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDzC2_aFxC7O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFshhaVkw_9S"
      },
      "source": [
        "Much more readable!\n",
        "In this simple case, the entire document is merely one short sentence.\n",
        "For each word in that sentence _spaCy_ has created a [token](https://spacy.io/api/token), and we accessed fields in each token to show:\n",
        "\n",
        " - raw text\n",
        " - [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)) – a dictionary form of the word\n",
        " - [part of speech](https://en.wikipedia.org/wiki/Part_of_speech)\n",
        " - a flag for whether the word is a _stopword_ – i.e., a common word that may be filtered out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCxlEULgwj1A"
      },
      "source": [
        "Next let's use the [displaCy](https://ines.io/blog/developing-displacy) library to visualize the parse tree for that sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZXwPB4vxgwV"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVT52Mouxn29"
      },
      "source": [
        "Does that bring back memories of grade school? Frankly, for those of us coming from more of a computational linguistics background, that diagram sparks joy.\n",
        "\n",
        "?? No ??\n",
        "\n",
        "But let's backup for a moment. How do you handle multiple sentences?\n",
        "\n",
        "There are features for _sentence boundary detection_ (SBD) – also known as _sentence segmentation_ – based on the builtin/default [sentencizer](https://spacy.io/api/sentencizer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFvA14--xqe1"
      },
      "outputs": [],
      "source": [
        "text = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in. Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(\">\", sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEh6vbAHx0KJ"
      },
      "source": [
        "When _spaCy_ creates a document, it uses a principle of _non-destructive tokenization_ meaning that the tokens, sentences, etc., are simply indexes into a long array. In other words, they don't carve the text stream into little pieces. So each sentence is a [span](https://spacy.io/api/span) with a _start_ and an _end_ index into the document array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4LVTWBnx2jt"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "for sent in doc.sents:\n",
        "    print(\">\", sent.start, sent.end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS_5uEVGyWMv"
      },
      "source": [
        "We can index into the document array to pull out the tokens for one sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOIdds_byV3A"
      },
      "outputs": [],
      "source": [
        "doc[48:54]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcq5W3wTyg7h"
      },
      "source": [
        "Or simply index into a specific token, such as the verb `went` in the last sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K-L5JnDykqj"
      },
      "outputs": [],
      "source": [
        "token = doc[51]\n",
        "print(token.text, token.lemma_, token.pos_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YTFDIlQynoo"
      },
      "source": [
        "At this point we can parse a document, segment that document into sentences, then look at annotations about the tokens in each sentence. That's a good start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeV__29z1DEy"
      },
      "source": [
        "# Side note about making text (more) readable\n",
        "First, a little housekeeping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqigoblH1mTp"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T67l53691vfG"
      },
      "source": [
        "## Now lets try to understand aliens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjNIZMkT10gS"
      },
      "outputs": [],
      "source": [
        "x = \"Rinôçérôse screams ﬂow not unlike an encyclopædia, \\\n",
        "'TECHNICIÄNS ÖF SPÅCE SHIP EÅRTH THIS IS YÖÜR CÄPTÅIN SPEÄKING YÖÜR ØÅPTÅIN IS DEA̋D' to Spın̈al Tap.\"\n",
        "\n",
        "type(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bemnEnFQ16Jg"
      },
      "outputs": [],
      "source": [
        "repr(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OimmKCE18AO"
      },
      "outputs": [],
      "source": [
        "ascii(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvHFsjq3196E"
      },
      "outputs": [],
      "source": [
        "x.encode('utf8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxeoVNk82AnC"
      },
      "outputs": [],
      "source": [
        "x.encode('ascii', 'ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plHD9ymJ2FW3"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "\n",
        "unicodedata.normalize('NFKD', x).encode('ascii','ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, this example is a little constructed. It likely does not happen often that people write `technicians` with `ä`. And in reality, we may use information. E.g., the german words `fallen` (to fall) and `fällen` (to fell) look the same with this processing step. Why is such a preprocessing useful nonetheless?"
      ],
      "metadata": {
        "id": "QVdjFHs8Mc7U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmJv2T5w2hhR"
      },
      "source": [
        "## Natural Language Understanding\n",
        "\n",
        "Now let's dive into some of the _spaCy_ features for NLU.\n",
        "Given that we have a parse of a document, from a purely grammatical standpoint we can pull the [noun chunks](https://spacy.io/usage/linguistic-features#noun-chunks), i.e., each of the noun phrases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET28hkH32m_Z"
      },
      "outputs": [],
      "source": [
        "text = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\n",
        "text2 = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit.\"\n",
        "doc = nlp(text2)\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYqTwWtJ2w3Y"
      },
      "source": [
        "Not bad. The noun phrases in a sentence generally provide more information content – as a simple filter used to reduce a long document into a more \"distilled\" representation.\n",
        "\n",
        "We can take this approach further and identify [named entities](https://spacy.io/usage/linguistic-features#named-entities) within the text, i.e., the proper nouns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpAvwOpW2zfh"
      },
      "outputs": [],
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzJdU3Xt21_E"
      },
      "outputs": [],
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(nlp(\"Bill Gates founded Microsoft\"), style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "maz4tBmxRdqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Parse Tree for Relation Classification\n",
        "The shortest dependency path (SDP) can be used for relation classification, the task of determining the relation expressed between to entities in a text. [[Xu et al. 2015](https://aclanthology.org/D15-1206/)]\n",
        "\n",
        "In the following text, notice how `causes` is the verb on the shortest (undirected) path between A and B in the dependence tree."
      ],
      "metadata": {
        "id": "ZkDguoZrLLyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(nlp(\"A certainly causes B\"), style=\"dep\", jupyter=True)\n",
        "displacy.render(nlp(\"He was born before 2000 in the capital of Germany, Berlin.\"), style=\"dep\", jupyter=True)\n",
        "displacy.render(nlp(\"He was born before 2000 in the capital of Germany, Berlin.\"), style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "_Bd_hRlPLbUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}