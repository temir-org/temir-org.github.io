{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdKmzJW0sLdU"
      },
      "source": [
        "# An Introduction to Natural Language Processing using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lttCa3MSLvPU"
      },
      "source": [
        "Import NLTK and download required resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02RtRYj_p0Xb"
      },
      "source": [
        "!pip install -qq svgling\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"book\", quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('maxent_ne_chunker_tab', quiet=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Misc"
      ],
      "metadata": {
        "id": "QaPhDlX5urYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Words"
      ],
      "metadata": {
        "id": "XGSDhguuutLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "F0L9YEWBo0ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "spZblt6LuvAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"went :\", lemmatizer.lemmatize(\"went\", wordnet.VERB))\n",
        "print(\"better (adjective):\", lemmatizer.lemmatize(\"better\", wordnet.ADJ))\n",
        "print(\"better (adverb):\", lemmatizer.lemmatize(\"better\", wordnet.ADV))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "def lemmatize_text(text: str):\n",
        "  def to_wordnet_pos(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "      return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "      return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "      return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "      return wordnet.ADV\n",
        "    else:\n",
        "      return wordnet.NOUN # Default value to avoid errors\n",
        "  words = nltk.word_tokenize(text)\n",
        "  tags = nltk.pos_tag(words)\n",
        "  for word, pos in tags:\n",
        "    yield lemmatizer.lemmatize(word, pos=to_wordnet_pos(pos))\n",
        "\n",
        "print(\" \".join(lemmatize_text(\"He sings better than before.\")))\n",
        "print(\" \".join(lemmatize_text(\"They better leave now.\")))"
      ],
      "metadata": {
        "id": "Qb4llXyFuqV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnfit4W3sTrc"
      },
      "source": [
        "## Tokenization, POS, Entities\n",
        "Take a sentence and tokenize into words. Then apply a part-of-speech tagger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOGPaeSbsdyk"
      },
      "source": [
        "sentence = \"\"\"At eight o'clock on Thursday morning Arthur Mills didn't feel very good.\"\"\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# Display it in a nice tree\n",
        "nltk.chunk.ne_chunk(tagged)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DwjDdIps3XS"
      },
      "source": [
        "## Concordance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENYr8J42s8sD"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY_MZaJzMy0o"
      },
      "source": [
        "Generate a key-word in context concordance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9iOak9GverQ"
      },
      "source": [
        "text1.concordance(\"monstrous\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oytCl9s7NAY2"
      },
      "source": [
        "Find words with similar concordance to a given word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd3K23bdvmkX"
      },
      "source": [
        "print(text1)\n",
        "text1.similar(\"monstrous\")\n",
        "print(text2)\n",
        "text2.similar(\"monstrous\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgmP2biDNIqU"
      },
      "source": [
        "Find contexts which are similar for the given words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A-UOoySwF1A"
      },
      "source": [
        "text2.common_contexts([\"monstrous\", \"very\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u09dCNVNVwt"
      },
      "source": [
        "Plot where in the text certain words appear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV4BVaUDwbJ_"
      },
      "source": [
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\", \"and\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhVpqEBtNaZv"
      },
      "source": [
        "Print the identity of a text, the length of the text and its vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amn1EmYDwrJ-"
      },
      "source": [
        "print(text3)\n",
        "print(len(text3))\n",
        "print(sorted(set(text3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS-0a_8kNiGY"
      },
      "source": [
        "Print some statistics of word occurrence in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca_e5fjQxOCG"
      },
      "source": [
        "def lexical_diversity(text):\n",
        "  return len(set(text)) / len(text)\n",
        "def percentage(count, total):\n",
        "  return 100 * count / total\n",
        "\n",
        "print(lexical_diversity(text3))\n",
        "print(lexical_diversity(text5))\n",
        "print(percentage(text4.count('a'), len(text4)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK in Action\n",
        "\n",
        "In the following, we will create a classifier that should tell us whether a movie review is positive or bad (\"sentiment analysis\")."
      ],
      "metadata": {
        "id": "gyQuWrFvWI3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Dataset\n",
        "\n",
        "To do so, we will be using the IMDB movie review corpus, which we can fetch through `nltk.corpus.movie_reviews.`:"
      ],
      "metadata": {
        "id": "wZGXVEsedEpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('movie_reviews', quiet=True)\n",
        "\n",
        "print(f\"Classes represented in the movie reviews: {movie_reviews.categories()}\")\n",
        "print(f\"#Documents: {sum(len(movie_reviews.fileids(cat)) for cat in movie_reviews.categories())}\")\n",
        "for cat in movie_reviews.categories():\n",
        "  print(f\"#Documents that are {cat}: {len(movie_reviews.fileids(cat))}\")\n",
        "\n",
        "# Write the dataset into a list[tuple[list[str], str]], where the first element\n",
        "# of the tuple is the document's text and the second element is its label (pos\n",
        "# or neg)\n",
        "documents = [(movie_reviews.words(fileid), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "print(f\"Average number of words per review: {(sum(len(w) for w,_ in documents))/len(documents)}\")\n",
        "print(f\"Average number of words per positive review: {(sum(len(w) for w,c in documents if c=='pos'))/len(movie_reviews.fileids('pos'))}\")\n",
        "print(f\"Average number of words per negative review: {(sum(len(w) for w,c in documents if c=='neg'))/len(movie_reviews.fileids('neg'))}\")\n",
        "## Draw histograms\n",
        "def render(dist: list, label: str, ax):\n",
        "  df = pd.DataFrame(dist)\n",
        "  df.plot.hist(bins=25, density=True, edgecolor='w', linewidth=0.5, ax=ax, alpha=0.4)\n",
        "  df.plot.density(color='k', alpha=0.5, ax=ax)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 4))\n",
        "ax.set_xlim((0, 4000))\n",
        "render((len(w) for w,c in documents if c=='pos'), \"positive\", ax)\n",
        "render((len(w) for w,c in documents if c=='neg'), \"negative\", ax)\n",
        "plt.ylabel(\"Density\")\n",
        "plt.xticks(np.arange(0, 4000, step=500))\n",
        "plt.xlabel(\"Wordcount\")\n",
        "ax.legend(labels=['Positive', '', 'Negative', ''], frameon=False)\n",
        "plt.savefig('score-density.pdf')\n",
        "plt.show()\n",
        "###\n",
        "\n",
        "\n",
        "# Features\n",
        "# There are many ways how we could represent the text. For now, we will choose\n",
        "# to represent it using a BOW model of the 2000 most common words in the dataset\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def normalize_words(words):\n",
        "  for w in words:\n",
        "    yield w.lower()\n",
        "\n",
        "all_words = nltk.FreqDist(normalize_words(movie_reviews.words()))\n",
        "common_words, _ = zip(*all_words.most_common(2000))\n",
        "print(f\"The most common words are: {' '.join(common_words[:10])} ...\")\n",
        "\n",
        "# Now that we have the most common words, we represent a document as a 2000\n",
        "# dimensional boolean vector.\n",
        "def doc_features(document: list[str]) -> dict[str, bool]:\n",
        "    docwords = set(document)\n",
        "    return np.fromiter(((word in docwords) for word in common_words), dtype=np.float32)\n",
        "\n",
        "dataset = [(doc_features(d), c) for (d, c) in documents]\n",
        "print(dataset[0])\n",
        "\n",
        "# Finally, we can split our dataset into training and testing splits\n",
        "train_set, test_set = train_test_split(dataset, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train, y_train = zip(*train_set)\n",
        "X_test, y_test = zip(*test_set)"
      ],
      "metadata": {
        "id": "RE6--GyKdEVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Logistic Regression"
      ],
      "metadata": {
        "id": "yGaWSxGYisoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train logistic regression\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "qDlMGIIYifH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coefs = clf.coef_[0]  # for binary classification\n",
        "\n",
        "def visualize_word_contributions(text_words):\n",
        "    # Create features for the input\n",
        "    X = doc_features(text_words).reshape(1, -1)\n",
        "\n",
        "    # Get prediction\n",
        "    prob = clf.predict_proba(X)[0]\n",
        "    pred = clf.predict(X)[0]\n",
        "\n",
        "    # Show contributions\n",
        "    contributions = [(word, coefs[i]) for i, word in enumerate(common_words) if word in text_words]\n",
        "\n",
        "    # Sort by magnitude\n",
        "    contributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "    contributions = contributions[:20]\n",
        "\n",
        "    # Plot\n",
        "    words, weights = zip(*contributions)\n",
        "    colors = ['green' if w > 0 else 'red' for w in weights]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.barh(words, weights, color=colors)\n",
        "    plt.xlabel(\"Contribution to Positive Sentiment\")\n",
        "    plt.title(f\"Prediction: {pred} (prob pos: {prob[1]:.2f})\")\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "example_text = list(movie_reviews.words(movie_reviews.fileids('pos')[0]))\n",
        "print(\" \".join(example_text))\n",
        "visualize_word_contributions(example_text)\n"
      ],
      "metadata": {
        "id": "PZwWMpO2jYN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = list(normalize_words(nltk.word_tokenize(\"The movie is bad\")))\n",
        "print(example_text)\n",
        "print(doc_features(example_text))\n",
        "visualize_word_contributions(example_text)"
      ],
      "metadata": {
        "id": "K1zA8edLuZat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = list(normalize_words(nltk.word_tokenize(\"The movie is good\")))\n",
        "print(example_text)\n",
        "print(doc_features(example_text))\n",
        "visualize_word_contributions(example_text)"
      ],
      "metadata": {
        "id": "7TtJemdwuSEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = list(normalize_words(nltk.word_tokenize(\"The movie could not have been better\")))\n",
        "print(example_text)\n",
        "print(doc_features(example_text))\n",
        "visualize_word_contributions(example_text)"
      ],
      "metadata": {
        "id": "2ztZQNQ6knJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions**\n",
        "- How could the effectiveness be improved further?\n",
        "\n",
        "\n",
        "**See Also:**\n",
        "- [Sentiment Flow â€“ A General Model of Web Review Argumentation](https://downloads.webis.de/publications/papers/wachsmuth_2015a.pdf); Wachsmuth et al. 2015"
      ],
      "metadata": {
        "id": "Q3WfiA_aqh7Z"
      }
    }
  ]
}