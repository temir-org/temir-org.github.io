{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generative LLMs for RAG\n",
    "\n",
    "This notebook provides a hands-on exploration of generative Large Language Models (LLMs). We'll start with cloud-based API models, and then explore how to run smaller models locally. Finally, we will explore different prompting strategies you can use to get best results.\n",
    "\n",
    "## Model Access & Generating an API Key\n",
    "\n",
    "For this lab, we recommend the BLABLADOR API provided by the German Supercomputing centre in JÃ¼lich. Follow [ðŸ”— their instructions](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/) to generate an API key. Use your university login to gain access. You can also interact via their [ðŸ”— Web UI](https://helmholtz-blablador.fz-juelich.de).\n",
    "\n",
    "They host several models, and you can specify the following alias names in API calls:\n",
    "- `alias-code` - Qwen2.5-Coder-7B-Instruct, a model that is specially trained for code.\n",
    "- `alias-embeddings` - GritLM-7B, a model specially made for embeddings\n",
    "- `alias-fast` - Ministral-8B-Instruct-2410, a model for high throughout (we will use this one in this lab)\n",
    "- `alias-large` - DeepSeek-R1-Distill-Llama-70B, a very large model; the most accurate, but also the slowest.\n",
    "- `alias-reasoning` - QwQ-32B, a model that is specially trained for reasoning.This model might not run 24h.\n",
    "\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Make sure to install the required libraries (comment out the following line, or make sure that your environment has these dependencies installe):\n"
   ],
   "id": "c6caf02762ddafad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:07.135910Z",
     "start_time": "2025-05-11T16:38:07.133568Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install openai torch transformers",
   "id": "e938b169c616d7d3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:07.263345Z",
     "start_time": "2025-05-11T16:38:07.261601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "API_URL = \"https://api.helmholtz-blablador.fz-juelich.de/v1/\"\n",
    "#API_KEY = \"<KEY>\"\n",
    "API_MODEL = \"alias-fast\" # Best for fast dev runs"
   ],
   "id": "cabb82c5f6869023",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calling LLMs via an API\n",
    "\n",
    "For the first step, we are interacting with an LLM via a hosted API. Most providers (BLABLADOR too) follow an Open-AI compliant API, meaning that you can use the `openai` python wrapper also to query models by non-openai providers. First, we create an API client object:"
   ],
   "id": "a1c50168102817db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:07.802157Z",
     "start_time": "2025-05-11T16:38:07.266042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=API_URL\n",
    ")"
   ],
   "id": "abe88f8e8f80b5f7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modern LLMs are usually finetuned for instructions, and their prompting follows a turn-based pattern: each message in a conversation with the LLM has a role associated with it (`user`, the user submitting the query; `system`, general instructions at the beginning of the conversation; or `assistant`, the reply of the LLM). For our first call, we are going to use the [`completions` API endpoint](https://platform.openai.com/docs/api-reference/chat/create), which you can use in python by calling `client.chat.completions.create`.\n",
    "\n",
    "It takes 2 mandatory arguments: the model (we are going to use `alias-fast`, saved in the `API_MODEL` variable), and the message, formatted as list of `{\"role\": <role>, \"content\": <message text>}` dictionaries."
   ],
   "id": "f923bc3ee5b9c8c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:09.856237Z",
     "start_time": "2025-05-11T16:38:07.807078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make a simple completion request\n",
    "response = client.chat.completions.create(\n",
    "    model=API_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain the concept of retrieval augmented generation in 2 sentences.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "e63946a77d7fbbe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieval Augmented Generation (RAG) is a method that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated text. It involves using a retrieval system to fetch relevant documents or information and then integrating that context into a generation model to produce more accurate and coherent responses.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Working with Temperature\n",
    "\n",
    "Of course, the API allows much more parameters to influence the results of the LLM generative process. First, `temperature`. Temperature controls \"randomness\" in the output, where `temperature = 0` yields a deterministic result, while `temperature = 1` yields a more unstable, but usually also more creative result. A balanced choice of e.g., `temperature = 0.7` is usually used.\n",
    "\n",
    "Try out how different temperature values affect the response generation for the same prompt:"
   ],
   "id": "5d6517674b293db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:11.969988Z",
     "start_time": "2025-05-11T16:38:09.869771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for temp in [0, 0.7, 1.0]:\n",
    "    response = client.chat.completions.create(\n",
    "        model=API_MODEL,\n",
    "        temperature=temp,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Write a single short advertising tagline for a retrieval augmented generation system.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ],
   "id": "4e7ecc9059c7c61c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Unlock the Power of Retrieval-Augmented Generation: Your Ideas, Our Expertise.\"\n",
      " \"Reach Beyond, Write Ahead!\"\n",
      " \"Transform Your Ideas, One Click at a Time.\"\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat Format: System, User, and Assistant Messages\n",
    "\n",
    "Instruction-tuned LLMs generally use different message roles, which we can leverage via the API:\n",
    "\n",
    "- **system**: sets behavior instructions, is usually passed as the first message in a chat to \"set the tone\"\n",
    "- **user**: represents human input messages, the prompt as you would enter it in an LLM\n",
    "- **assistant**: represents AI responses, the generated text received by the LLM (usually from an earlier conversation turn)\n",
    "\n",
    "Try out different system prompts where you command the model to take on different personas to see how its reponse to the actual prompt differs."
   ],
   "id": "c4dcf60b3376c9e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:14.017492Z",
     "start_time": "2025-05-11T16:38:11.977572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are a technical expert who explains concepts briefly in only a few sentences.\"\n",
    "user_prompt = \"What is retrieval-augmented generation?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=API_MODEL,\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "b75b34442a2c6126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieval-augmented generation (RAG) is a technique where a model retrieves relevant documents or information from an external knowledge base or index before generating a response. This helps to ensure that the generated content is accurate, up-to-date, and based on factual information. The retrieved data is then used to enhance the model's understanding and improve the quality of the output.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:16.210863Z",
     "start_time": "2025-05-11T16:38:14.044040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are a preschool teacher explaining concepts to children. The focus is on being short and accessible.\"\n",
    "user_prompt = \"What is retrieval-augmented generation?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "67e2ea6ad88bdeb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieval-augmented generation is a method where you combine the power of retrieval-based and generative models to create more informative and accurate responses. It's like having a smart librarian who can quickly find the best information, and then a skilled storyteller who can explain it in a clear and engaging way. This combination helps provide more precise and helpful information to the people you're communicating with.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-turn Conversations\n",
    "\n",
    "LLMs can maintain context across multiple messages. Via the API, you can simulate conversations by appending responses and your own follow-up message to the message list. Generated responses are inserted using the `assistant` role, and the follow-up user message is then posed with the `user` role.\n",
    "\n",
    "Ask the LLM a question, append its response, and then ask a follow-up question to see how it maintains context across the whole conversation.\n"
   ],
   "id": "31d63c3414889e3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:16.237062Z",
     "start_time": "2025-05-11T16:38:16.233331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant providing assistance to users by guiding their learning process. You answer in short, to-the-point complete answers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I want to learn about natural language processing and information retrieval. How should I start?\"}\n",
    "]"
   ],
   "id": "c04ec8d5b1723e42",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:25.057747Z",
     "start_time": "2025-05-11T16:38:16.249080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First turn\n",
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "assistant_response = response.choices[0].message.content\n",
    "print(\"Assistant:\", assistant_response)"
   ],
   "id": "b74491492796298f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Great choice! Hereâ€™s how you can start learning about Natural Language Processing (NLP) and Information Retrieval (IR):\n",
      "1. **Foundational Math & Computing**:\n",
      "   - Brush up on your knowledge in statistics, machine learning, and basic programming (especially Python).\n",
      "\n",
      "2. **NLP Basics**:\n",
      "   - Read introductory texts like \"Speech and Language Processing\" by Jurafsky and Martin.\n",
      "   - Learn about tokenization, parsing, named entity recognition, sentiment analysis, and more.\n",
      "\n",
      "3. **Online Courses**:\n",
      "   - **Coursera**: \"Natural Language Processing Specialization\" by Deeplearning.ai.\n",
      "   - **edX**: \"Natural Language Processing with Classification and Vector Spaces\" by Harvard.\n",
      "\n",
      "4. **Practice**:\n",
      "   - Work on projects using datasets available on Kaggle or from NLP libraries like NLTK (Natural Language Toolkit).\n",
      "\n",
      "5. **Information Retrieval**:\n",
      "   - Start with understanding the basics of IR, such as tf-idf, BM25, and relevance feedback.\n",
      "   - Explore search engines and their algorithms, like those used by Google or Bing.\n",
      "\n",
      "6. **Further Reading**:\n",
      "   - Books like \"Introduction to Information Retrieval\" by Christopher D. Manning, Prabhakar Raghavan, and Hinrich SchÃ¼tze.\n",
      "   - Research papers on topics of interest, available on arXiv.\n",
      "\n",
      "7. **Practice and Projects**:\n",
      "   - Implement your own retrieval system, possibly using tools and libraries such as Elasticsearch or Apache Solr.\n",
      "   - Participate in Kaggle competitions related to NLP and IR.\n",
      "\n",
      "8. **Stay Updated**:\n",
      "   - Follow relevant blogs, attend webinars, and keep an eye on recent developments in NLP and IR.\n",
      "\n",
      "By starting with these steps, you can build a strong foundation in both NLP and IR. Good luck!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:25.085352Z",
     "start_time": "2025-05-11T16:38:25.081748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add the response to the conversation history; notice the 'assistant'  role\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "# Second turn - add your follow-up message; notice the 'user'  role\n",
    "messages.append({\"role\": \"user\", \"content\": \"What Python libraries should I use for that?\"})"
   ],
   "id": "96857cdd2811e3a7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:26.319160Z",
     "start_time": "2025-05-11T16:38:25.100260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "f4e1a2046e208761",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some key Python libraries for Natural Language Processing (NLP) and Information Retrieval (IR):\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running Local LLMs\n",
    "\n",
    "Besides calling models via an API, you might want to run models locally, for example if you lack internet access, for experimentation without worrying about rate limits or API cost, or when working with privacy-sensitive data. In the following, we will use models loaded from the [Huggingface]() model repository and use them with the `transformers` library. As a rule of thumb, model below 400M parameters usually fit in 16GB RAM, with acceptable inference times on CPU."
   ],
   "id": "6811982572faac7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll use a small model that can run on a CPU: [`HuggingFaceTB/SmolLM2-360M-Instruct`](). We need both the model itself, and a tokenizer to convert the message blocks into token IDs the model can consume.",
   "id": "93c0952697e0f3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:31.446836Z",
     "start_time": "2025-05-11T16:38:26.345461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "device = \"cpu\" # \"gpu\" for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ],
   "id": "ca88165c4ea68e8d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gienapp/Gits/Teaching/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the tokenizer to turn the message list into usable model inputs. The [`tokenizer.apply_chat_template`]() function can directly produce the desired result. Important: specify the arguments `tokenize=True` and `return_tensors=\"pt\"` to get the correct Torch tensor objects for the model. Also remember to send the tokenized data to the same device as the model using the `.to(device)` method as already with the model.",
   "id": "42e285ee55be4340"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:31.506167Z",
     "start_time": "2025-05-11T16:38:31.486743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the message as before\n",
    "messages = messages # Or replace with a new converstation\n",
    "# Tokenize the message\n",
    "inputs=tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\").to(device)"
   ],
   "id": "f6930572a5191b42",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you can call the `model.generate()` function with the tokenized inputs to produce generated text. However, the model returns token IDs, not their string representation. You can convert the model output back into human-readable format using the `tokenizer.decode()` function.",
   "id": "92d40ee58b8bc867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:39:00.504660Z",
     "start_time": "2025-05-11T16:38:31.523641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the model inference pass\n",
    "outputs = model.generate(inputs, max_new_tokens=500)\n",
    "# Decode the output message\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "799977a168404cb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant providing assistance to users by guiding their learning process. You answer in short, to-the-point complete answers.<|im_end|>\n",
      "<|im_start|>user\n",
      "I want to learn about natural language processing and information retrieval. How should I start?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " Great choice! Hereâ€™s how you can start learning about Natural Language Processing (NLP) and Information Retrieval (IR):\n",
      "1. **Foundational Math & Computing**:\n",
      "   - Brush up on your knowledge in statistics, machine learning, and basic programming (especially Python).\n",
      "\n",
      "2. **NLP Basics**:\n",
      "   - Read introductory texts like \"Speech and Language Processing\" by Jurafsky and Martin.\n",
      "   - Learn about tokenization, parsing, named entity recognition, sentiment analysis, and more.\n",
      "\n",
      "3. **Online Courses**:\n",
      "   - **Coursera**: \"Natural Language Processing Specialization\" by Deeplearning.ai.\n",
      "   - **edX**: \"Natural Language Processing with Classification and Vector Spaces\" by Harvard.\n",
      "\n",
      "4. **Practice**:\n",
      "   - Work on projects using datasets available on Kaggle or from NLP libraries like NLTK (Natural Language Toolkit).\n",
      "\n",
      "5. **Information Retrieval**:\n",
      "   - Start with understanding the basics of IR, such as tf-idf, BM25, and relevance feedback.\n",
      "   - Explore search engines and their algorithms, like those used by Google or Bing.\n",
      "\n",
      "6. **Further Reading**:\n",
      "   - Books like \"Introduction to Information Retrieval\" by Christopher D. Manning, Prabhakar Raghavan, and Hinrich SchÃ¼tze.\n",
      "   - Research papers on topics of interest, available on arXiv.\n",
      "\n",
      "7. **Practice and Projects**:\n",
      "   - Implement your own retrieval system, possibly using tools and libraries such as Elasticsearch or Apache Solr.\n",
      "   - Participate in Kaggle competitions related to NLP and IR.\n",
      "\n",
      "8. **Stay Updated**:\n",
      "   - Follow relevant blogs, attend webinars, and keep an eye on recent developments in NLP and IR.\n",
      "\n",
      "By starting with these steps, you can build a strong foundation in both NLP and IR. Good luck!<|im_end|>\n",
      "<|im_start|>user\n",
      "What Python libraries should I use for that?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "For NLP and IR, here are some Python libraries you might find useful:\n",
      "\n",
      "1. **NLTK (Natural Language Toolkit)**: A popular library for NLP tasks, it provides tools for tokenization, part-of-speech tagging, named entity recognition, and more.\n",
      "\n",
      "2. **Stanford CoreNLP**: A comprehensive library for NLP tasks, it includes tools for tokenization, part-of-speech tagging, named entity recognition, and more.\n",
      "\n",
      "3. **spaCy**: A fast and expressive language model library, it provides tools for tokenization, part-of-speech tagging, named entity recognition, and more.\n",
      "\n",
      "4. **PyTorch**: A popular deep learning library, it provides tools for NLP tasks like tokenization, part-of-speech tagging, named entity recognition, and more.\n",
      "\n",
      "5. **scikit-learn**: A machine learning library, it provides tools for tasks like classification, regression, clustering, and more.\n",
      "\n",
      "6. **Google's NLP API**: A Python API for Google's NLP API, it provides tools for tasks like tokenization, part-of-speech tagging, named entity recognition, and more.\n",
      "\n",
      "7. **Apache Spark**: A big data processing library, it provides tools for NLP tasks like tokenization, part-of-speech tagging, named entity recognition, and more.\n",
      "\n",
      "8. **Keras**: A high-level neural networks API, it provides tools for tasks like tokenization, part-of-speech tagging, named entity recognition, and more.\n",
      "\n",
      "Remember, the choice of library depends on your specific needs and the type of tasks you want to perform.<|im_end|>\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "To get the most out of any LLM, you need to carefully design prompts. Many prompting techniques exist and are hot topic in current research. The guides linked below provide a comprehensive overview on prompt engineering. In this notebook, we will explore three prompt engineering techniques:\n",
    "\n",
    "- Prompting-Induced Planning / Chain-of-Thought\n",
    "- Self-critique\n",
    "- Structured output prompting\n",
    "\n",
    "The following guides explore prompt engineering techniques in more detail:\n",
    "\n",
    "- [ðŸ”— Prompt Engineering Guide](https://drive.google.com/file/d/1AbaBYbEa_EbPelsT40-vj64L-2IwUJHy/view)\n",
    "- [ðŸ”— GPT4.1 Prompting Guide](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)\n"
   ],
   "id": "7a2523ef3c9a0806"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For prompt engineering, we will consider the RAG usecase with the example query and retrieved snippets below. The goal is to provide a short but informative answer to the user.",
   "id": "96da5397faf5bf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:39:00.523973Z",
     "start_time": "2025-05-11T16:39:00.520115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"why has olive oil increased in price\"\n",
    "snippets = [\n",
    "    (\"Global olive oil prices have surged due to poor harvests in Spain and Italy, caused by extreme drought and heatwaves linked to climate change.\", 0.983),\n",
    "    (\"A sharp decline in olive oil production, especially in major producing countries like Spain, has led to reduced supply and increased prices worldwide.\", 0.942),\n",
    "    (\"Increased production costs, including higher labor and transportation expenses, have contributed to the rise in olive oil prices.\", 0.891),\n",
    "    (\"Rising inflation and currency fluctuations have made imported goods, including olive oil, more expensive in several countries.\", 0.847),\n",
    "    (\"Retailers report that consumer demand for premium oils has increased, indirectly pushing prices higher across all olive oil grades.\", 0.812),\n",
    "    (\"Climate change has disrupted agricultural cycles in the Mediterranean region, impacting many crops including olives.\", 0.768),\n",
    "    (\"Sunflower oil shortages due to the war in Ukraine have led to a shift in demand toward olive oil, tightening global supply.\", 0.703),\n",
    "    (\"The Mediterranean diet, which emphasizes olive oil consumption, continues to gain popularity for its health benefits.\", 0.623),\n",
    "    (\"Spain is one of the world's largest producers of olive oil, exporting millions of liters each year.\", 0.578),\n",
    "    (\"Olive trees can live for hundreds of years and are cultivated mostly in Mediterranean climates.\", 0.519)\n",
    "]"
   ],
   "id": "457c33befff9a2f9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vanilla RAG\n",
    "\n",
    "The most basic RAG prompting technique directly feeds retrieved context into the model alongside the query. This approach serves as a baseline: it simply instructs the model to answer the question based on the given context, without extra guidance.\n",
    "\n",
    "Implement the vanilla RAG prompt using a simple format with a system message and a user message containing the context and question."
   ],
   "id": "9cf4ddac0541163"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:39:06.642538Z",
     "start_time": "2025-05-11T16:39:00.547577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are a helpful assistant answering a question based on retrieved context information.\"\n",
    "user_prompt = f\"\"\"\n",
    "Context:\n",
    "{\"\\n- \".join([text for text, score in snippets])}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "a2c596b01714aff1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Olive oil prices have increased due to several factors:\n",
      "1. **Poor Harvests**: Extreme drought and heatwaves linked to climate change have led to poor harvests in major olive oil-producing countries like Spain and Italy, reducing the supply of olive oil.\n",
      "2. **Increased Production Costs**: Higher labor and transportation expenses have contributed to increased costs of production, driving up olive oil prices.\n",
      "3. **Inflation and Currency Fluctuations**: Rising inflation and fluctuations in currency exchange rates have made imported goods, including olive oil, more expensive in several countries.\n",
      "4. **Shift in Demand**: The increasing demand for premium olive oils due to the popularity of the Mediterranean diet has indirectly pushed prices higher across all olive oil grades.\n",
      "5. **Sunflower Oil Shortages**: The war in Ukraine has led to a shortage of sunflower oil, causing a shift in demand toward olive oil and further tightening the global supply.\n",
      "6. **Climate Change**: The impact of climate change on agricultural cycles in the Mediterranean region has disrupted the growth and yield of olive trees, affecting olive oil production.\n",
      "\n",
      "These factors have combined to create a perfect storm, leading to a significant increase in olive oil prices worldwide.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Chain-of-thought prompting encourages the model to reason step by step before producing a final answer. This helps improve factual accuracy and clarity, especially when the retrieved context is complex or contains multiple causal links.\n",
    "\n",
    "Implement CoT prompting by asking the model to first identify relevant information, then reason through the answer, and finally summarize the conclusion."
   ],
   "id": "91862c6c096de2fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:39:11.585452Z",
     "start_time": "2025-05-11T16:39:06.670833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are a smart assistant answering a question using the provided context. First, identify which parts of the context are most relevant to the question. Then, briefly reason through the answer using only the relevant information. Finally, provide a short and informative answer.\"\n",
    "\n",
    "user_prompt = user_prompt # Same as before\n",
    "\n",
    "assistant_prompt = \"\"\"\n",
    "Let's think step by step:\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "ce596fd4c68c4f79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The price of olive oil has increased primarily due to:\n",
      "1. **Poor Harvests**: The extreme drought and heatwaves in Spain and Italy have led to reduced olive oil production, as olives are not ripening as they normally would. This decrease in supply has driven up prices.\n",
      "2. **Increased Production Costs**: Higher labor and transportation expenses have contributed to the rise in prices.\n",
      "3. **Rising Inflation and Currency Fluctuations**: These factors have made imported goods, including olive oil, more expensive.\n",
      "4. **Shift in Demand**: With sunflower oil shortages due to the war in Ukraine, there has been a shift in demand toward olive oil, further tightening global supply.\n",
      "5. **Popularity of the Mediterranean Diet**: The increasing popularity of the Mediterranean diet, which emphasizes olive oil consumption, has also contributed to the higher demand and prices.\n",
      "\n",
      "So, the main reasons for the increase in olive oil prices are reduced supply due to poor harvests and increased demand.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Critique-and-Revise Prompting\n",
    "\n",
    "To improve factuality or clarity, we can instruct the model to critique an initial answer before revising it. This multi-step prompting encourages reflection and refinement, and can lead to higher-quality outputs.\n",
    "\n",
    "Generate an initial answer, prompt the model to critique it, and then ask for a revised version based on that critique."
   ],
   "id": "d9495d633540af3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:39:44.933768Z",
     "start_time": "2025-05-11T16:39:11.610293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###### Self-critique\n",
    "\n",
    "initial_response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages # Same as before\n",
    ")\n",
    "\n",
    "print(\"Initial response:\", initial_response.choices[0].message.content)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Critique this initial response, providing suggestions on how to make it better.\"})\n",
    "\n",
    "critique = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"\\n\\n Critique:\", critique.choices[0].message.content)\n",
    "messages.append({\"role\": \"assistant\", \"content\": critique.choices[0].message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Answer the question using initial response, but take into account the suggestions.\"})\n",
    "\n",
    "final_response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"\\n\\n Final response:\", final_response.choices[0].message.content)"
   ],
   "id": "9a7ef76a26c1d42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response:  Here are some essential Python libraries for Natural Language Processing (NLP) and Information Retrieval (IR):\n",
      "1. **NLP Libraries**:\n",
      "   - **NLTK (Natural Language Toolkit)**: A comprehensive library for NLP tasks.\n",
      "   - **SpaCy**: Highly efficient for advanced NLP tasks.\n",
      "   - **Gensim**: Useful for topic modeling and document similarity.\n",
      "   - **Transformers by Hugging Face**: State-of-the-art pre-trained models for various NLP tasks.\n",
      "   - **TextBlob**: Simplifies common NLP tasks like part-of-speech tagging, noun phrase extraction, sentiment analysis, and more.\n",
      "\n",
      "2. **IR Libraries**:\n",
      "   - **Whoosh**: An indexing and search library implemented in pure Python.\n",
      "   - **Elasticsearch**: A powerful open-source search and analytics engine.\n",
      "   - **Apache Solr**: A highly reliable and scalable search platform.\n",
      "   - **Rank-BM25**: A Python library for BM25 scoring algorithm.\n",
      "\n",
      "3. **Data Preprocessing and Text Analysis**:\n",
      "   - **Pandas**: For data manipulation and analysis.\n",
      "   - **NumPy**: For numerical computations.\n",
      "   - **SciPy**: For scientific and technical computing.\n",
      "\n",
      "4. **Machine Learning**:\n",
      "   - **Scikit-Learn**: For basic machine learning tasks.\n",
      "   - **XGBoost**: For gradient boosting algorithms.\n",
      "\n",
      "5. **Visualization**:\n",
      "   - **Matplotlib**: For plotting and visualizing data.\n",
      "   - **Seaborn**: For statistical data visualization.\n",
      "\n",
      "6. **Web Scraping**:\n",
      "   - **Beautiful Soup**: For parsing HTML and XML documents.\n",
      "   - **Scrapy**: For web scraping and crawling.\n",
      "\n",
      "Using these libraries will help you build robust NLP and IR systems. Start with the basics and gradually incorporate more complex tools as you become more proficient.\n",
      "\n",
      "\n",
      " Critique:  The initial response provided a good summary of the reasons behind the olive oil price increase. Here's a critique and some suggestions to improve it:\n",
      "\n",
      "**Critique:**\n",
      "- The response only lists the reasons without providing context or explaining the interconnectedness of these factors.\n",
      "- It lacks specific details and examples that could clarify the points.\n",
      "- The suggestions for further action are missing.\n",
      "\n",
      "**Suggestions:**\n",
      "1. **Context and Interconnectedness**: Provide a brief explanation of how these factors are interconnected. For example, explain how the war in Ukraine impacts olive oil supply and why that is significant.\n",
      "2. **Specific Details and Examples**: Incorporate specific details and examples to make the response more informative. For example, mention the amount of olive oil produced in affected countries and the potential impacts on global supply.\n",
      "3. **Stated Implications**: Clearly state the implications of these factors. For instance, explain how the poor harvests in Spain and Italy are expected to affect global prices and how that might impact consumers.\n",
      "4. **Further Action**: Suggest potential follow-up actions or further research that could be done to better understand or address the issue. For example, suggest looking into alternative oil sources or sustainable farming practices.\n",
      "\n",
      "**Revised Response:**\n",
      "The price of olive oil has increased due to a combination of factors:\n",
      "1. **Poor Harvests**: The extreme drought and heatwaves in Spain and Italy have led to a significant reduction in olive oil production. Spain, the world's largest olive oil producer, has seen a decrease in production by approximately 25%, while Italy has reported a 30% drop in production. This reduced supply has led to a significant increase in olive oil prices.\n",
      "2. **Increased Production Costs**: The rising costs of labor and transportation have also contributed to the higher olive oil prices. Farmers are facing higher labor costs due to labor shortages and increased transportation costs due to fuel price hikes.\n",
      "3. **Rising Inflation and Currency Fluctuations**: Inflation and currency fluctuations have made imported goods, including olive oil, more expensive. The devaluation of the euro and the strengthening of the U.S. dollar have made European olive oil more expensive for American consumers.\n",
      "4. **Shift in Demand**: The ongoing sunflower oil shortages due to the war in Ukraine have led to a shift in demand toward olive oil, further tightening global supply. This increased demand has put more pressure on the already reduced supply of olive oil.\n",
      "5. **Popularity of the Mediterranean Diet**: The growing popularity of the Mediterranean diet, which emphasizes olive oil consumption, has also contributed to the higher demand and prices. The demand for olive oil has been consistently increasing over the past decade, driven by health-conscious consumers.\n",
      "\n",
      "These interconnected factors have significantly impacted the global olive oil market, leading to a substantial increase in prices. To better understand and mitigate these issues, further research into sustainable farming practices, alternative oil sources, and the potential impacts of the war in Ukraine on global food supplies could be beneficial. Additionally, consumers might consider exploring other oil sources or adjusting their consumption habits to adapt to the current market conditions.\n",
      "\n",
      "\n",
      " Final response:  The price of olive oil has increased primarily due to several interconnected factors, including poor harvests in major producing countries like Spain and Italy, increased production costs, rising inflation and currency fluctuations, a shift in demand toward olive oil due to sunflower oil shortages, and the growing popularity of the Mediterranean diet.\n",
      "\n",
      "To better understand these factors and their impacts:\n",
      "- **Poor Harvests**: The severe drought and heatwaves in Spain and Italy have led to a substantial reduction in olive oil production. Spain, the world's largest olive oil producer, has seen a decrease in production by approximately 25%, while Italy has reported a 30% drop. This reduced supply has significantly contributed to the price increase.\n",
      "- **Increased Production Costs**: Higher labor and transportation expenses have also driven up the cost of olive oil. Farmers are facing rising labor costs due to labor shortages and increased transportation costs due to fuel price hikes.\n",
      "- **Rising Inflation and Currency Fluctuations**: Inflation and currency fluctuations have made imported goods more expensive. The devaluation of the euro and the strengthening of the U.S. dollar have led to higher prices for European olive oil in the American market.\n",
      "- **Shift in Demand**: The ongoing sunflower oil shortages due to the war in Ukraine have shifted demand toward olive oil, further tightening global supply and putting more pressure on the already reduced supply of olive oil.\n",
      "- **Popularity of the Mediterranean Diet**: The increasing popularity of the Mediterranean diet, which emphasizes olive oil consumption, has contributed to the higher demand and prices. The demand for olive oil has been consistently rising over the past decade, driven by health-conscious consumers.\n",
      "\n",
      "These interconnected factors have significantly impacted the global olive oil market, leading to a substantial price increase. To better understand and mitigate these issues, further research into sustainable farming practices, alternative oil sources, and the potential impacts of the war in Ukraine on global food supplies could be beneficial. Additionally, consumers might consider exploring other oil sources or adjusting their consumption habits to adapt to the current market conditions.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Self-Consistency\n",
    "\n",
    "As we have seen before, a model might produce different outputs at repeated inference when using higher temperatures. We can use this to our advantage by prompting the model for self-consistency: first, we generate several candidate answers at high temperature (leveraging the more creative, diverse output), and then provide these to the model to distill into a final answer at low temperature (yielding consistent output).\n",
    "\n",
    "Implement self-consistency by generating 3 candidates at high temperature, and combining them in a follow-up inference pass. Design special prompts for both phases."
   ],
   "id": "9819b471157ffdf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:39:58.414447Z",
     "start_time": "2025-05-11T16:39:44.959979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_candidates = 3\n",
    "candidate_responses = []\n",
    "for i in range(num_candidates):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "    ]\n",
    "    response_candidate = client.chat.completions.create(\n",
    "        model=\"alias-fast\",\n",
    "        temperature=0.9,\n",
    "        messages=messages\n",
    "    )\n",
    "    candidate_responses.append(response_candidate.choices[0].message.content)\n",
    "\n",
    "print(candidate_responses)"
   ],
   "id": "1080f7da91133d44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Olive oil prices have increased due to several factors:\\n\\n1. **Poor Harvests in Major Producing Countries**: Spain and Italy, two major producers, have experienced poor harvests due to extreme drought and heatwaves. These natural disasters have significantly reduced olive oil supply.\\n2. **Increased Production Costs**: Higher labor and transportation expenses have contributed to the rise in olive oil prices.\\n3. **Inflation and Currency Fluctuations**: The rising inflation and unstable currency values have made imported goods, including olive oil, more expensive worldwide.\\n4. **Consumer Demand for Premium Oils**: Increased demand for premium olive oils has indirectly pushed prices higher across all grades.\\n5. **Climate Change Impact**: Climate change has disrupted traditional agricultural cycles, exacerbating supply issues for crops like olives.\\n6. **Shift in Demand due to Sunflower Oil Shortages**: The war in Ukraine has led to sunflower oil shortages, causing consumers to shift their demand to olive oil, which has increased global supply pressure.\\n\\nTherefore, the combination of these factorsâ€”reduced supply, increased production and transportation costs, currency fluctuations, consumer demand, and climate impactsâ€”has led to a significant increase in olive oil prices.', ' Olive oil prices have increased due to several factors:\\n1. **Poor Harvests**: Extreme drought and heatwaves in Spain and Italy, caused by climate change, have led to a sharp decline in olive oil production. This reduction in supply has driven up prices.\\n2. **Increased Production Costs**: Higher labor and transportation expenses have contributed to the rise in prices. These costs have been exacerbated by rising inflation and currency fluctuations, making it more expensive to import oil.\\n3. **Consumer Demand**: The increasing popularity of the Mediterranean diet, which relies heavily on olive oil, has led to higher consumer demand. This increased demand for premium oils has indirectly pushed prices higher across all grades.\\n4. **Supply and Demand Shifts**: The war in Ukraine and resulting sunflower oil shortages have led to a shift in demand toward olive oil, further tightening the global supply and increasing prices.\\n\\nThese factors combined have driven up the price of olive oil worldwide.', ' The most relevant parts of the context are:\\n1. \"Global olive oil prices have surged due to poor harvests in Spain and Italy, caused by extreme drought and heatwaves linked to climate change.\" - The poor harvests due to extreme weather conditions linked to climate change have significantly reduced the supply of olive oil, leading to higher prices.\\n2. \"Retailers report that consumer demand for premium oils has increased\" - Increased demand for high-quality olive oil has put pressure on prices, even at a premium grade level.\\n\\n']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:40:00.337242Z",
     "start_time": "2025-05-11T16:39:58.440779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a smart assistant answering a question based on provided candidate answers. Provide a short and informative definitive answer with only the most relevant information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\\n\\n\\n\".join([f\"Suggestion {i}: {text}\" for i, text in enumerate(candidate_responses)])},\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "62f1eb38b4ea1072",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Olive oil prices have increased due to poor harvests in major producing countries like Spain and Italy, caused by extreme drought and heatwaves linked to climate change. This reduction in supply, combined with increased production costs, currency fluctuations, and higher consumer demand for premium oils, has driven up prices worldwide.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Structured Output\n",
    "\n",
    "For downstream applications, you should have the model return structured outputs, e.g., in JSON dictionaries. Then, you can elicit responses that decompose their answer into different aspects. For example, you could refine the critique-and-revise technique from before my having the model return the critique and the final response as fields in a JSON dictionary, or separate reasoning and response in the answer in order to only display the response.\n",
    "\n",
    "For RAG, you can also use structured outputs to have the model attribute its reasoning to passages. Try to make it return its answer in a list of sentences, where for each sentence is represented as JSON dictionary with the text and the relevant passages information is taken from."
   ],
   "id": "6bbc16ee3c1ce782"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:40:00.365599Z",
     "start_time": "2025-05-11T16:40:00.361953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an assistant that answers questions based on retrieved context. Generate an informative but succinct answer to the question.\n",
    "Organize your answer as a list of JSON dictionaries, one for each answer passage, in the following format:\n",
    "\n",
    "[\n",
    "    {\n",
    "    \"text\": \"\", # The passage text\n",
    "    \"ref\": [1,2] # The indices of sources used in that text\n",
    "    },\n",
    "    ... # More passages\n",
    "]\n",
    "\n",
    "Answer only with valid JSON.\n",
    "\"\"\"\n",
    "context = \"\\n\".join([f\"({i}) {text}\" for i, (text, score) in enumerate(snippets)])\n",
    "user_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n"
   ],
   "id": "dd520cf1a42cb7dc",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:40:07.231410Z",
     "start_time": "2025-05-11T16:40:00.401110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "197c6163ecfdc912",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Olive oil prices have increased due to several factors:\n",
      "[\n",
      "    {\n",
      "    \"text\": \"Global olive oil prices have surged due to poor harvests in Spain and Italy, caused by extreme drought and heatwaves linked to climate change.\",\n",
      "    \"ref\": [0]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"A sharp decline in olive oil production, especially in major producing countries like Spain, has led to reduced supply and increased prices worldwide.\",\n",
      "    \"ref\": [1]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Increased production costs, including higher labor and transportation expenses, have contributed to the rise in olive oil prices.\",\n",
      "    \"ref\": [2]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Rising inflation and currency fluctuations have made imported goods, including olive oil, more expensive in several countries.\",\n",
      "    \"ref\": [3]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Retailers report that consumer demand for premium oils has increased, indirectly pushing prices higher across all olive oil grades.\",\n",
      "    \"ref\": [4]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Climate change has disrupted agricultural cycles in the Mediterranean region, impacting many crops including olives.\",\n",
      "    \"ref\": [5]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Sunflower oil shortages due to the war in Ukraine have led to a shift in demand toward olive oil, tightening global supply.\",\n",
      "    \"ref\": [6]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:40:07.242153Z",
     "start_time": "2025-05-11T16:40:07.239346Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "250565dd8752f6a8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
