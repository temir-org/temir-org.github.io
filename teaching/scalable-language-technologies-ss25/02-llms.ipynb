{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generative LLMs for RAG\n",
    "\n",
    "This notebook provides a hands-on exploration of generative Large Language Models (LLMs). We'll start with cloud-based API models, and then explore how to run smaller models locally. Finally, we will explore different prompting strategies you can use to get best results.\n",
    "\n",
    "## Model Access & Generating an API Key\n",
    "\n",
    "For this lab, we recommend the BLABLADOR API provided by the German Supercomputing centre in JÃ¼lich. Follow [ðŸ”— their instructions](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/) to generate an API key. Use your university login to gain access. You can also interact via their [ðŸ”— Web UI](https://helmholtz-blablador.fz-juelich.de).\n",
    "\n",
    "They host several models, and you can specify the following alias names in API calls:\n",
    "- `alias-code` - Qwen2.5-Coder-7B-Instruct, a model that is specially trained for code.\n",
    "- `alias-embeddings` - GritLM-7B, a model specially made for embeddings\n",
    "- `alias-fast` - Ministral-8B-Instruct-2410, a model for high throughout (we will use this one in this lab)\n",
    "- `alias-large` - DeepSeek-R1-Distill-Llama-70B, a very large model; the most accurate, but also the slowest.\n",
    "- `alias-reasoning` - QwQ-32B, a model that is specially trained for reasoning.This model might not run 24h.\n",
    "\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Make sure to install the required libraries (comment out the following line, or make sure that your environment has these dependencies installe):\n"
   ],
   "id": "c6caf02762ddafad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#!pip install openai torch transformers",
   "id": "e938b169c616d7d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "API_URL = \"https://api.helmholtz-blablador.fz-juelich.de/v1/\"\n",
    "API_KEY = \"<KEY>\"\n",
    "API_MODEL = \"alias-fast\" # Best for fast dev runs"
   ],
   "id": "cabb82c5f6869023",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calling LLMs via an API\n",
    "\n",
    "For the first step, we are interacting with an LLM via a hosted API. Most providers (BLABLADOR too) follow an Open-AI compliant API, meaning that you can use the `openai` python wrapper also to query models by non-openai providers. First, we create an API client object:"
   ],
   "id": "a1c50168102817db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=API_URL\n",
    ")"
   ],
   "id": "abe88f8e8f80b5f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modern LLMs are usually finetuned for instructions, and their prompting follows a turn-based pattern: each message in a conversation with the LLM has a role associated with it (`user`, the user submitting the query; `system`, general instructions at the beginning of the conversation; or `assistant`, the reply of the LLM). For our first call, we are going to use the [`completions` API endpoint](https://platform.openai.com/docs/api-reference/chat/create), which you can use in python by calling `client.chat.completions.create`.\n",
    "\n",
    "It takes 2 mandatory arguments: the model (we are going to use `alias-fast`, saved in the `API_MODEL` variable), and the message, formatted as list of `{\"role\": <role>, \"content\": <message text>}` dictionaries."
   ],
   "id": "f923bc3ee5b9c8c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make a simple completion request\n",
    "response = client.chat.completions.create(\n",
    "    model=API_MODEL,\n",
    "    messages=[\n",
    "        # <your messages>\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "e63946a77d7fbbe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Working with Temperature\n",
    "\n",
    "Of course, the API allows much more parameters to influence the results of the LLM generative process. First, `temperature`. Temperature controls \"randomness\" in the output, where `temperature = 0` yields a deterministic result, while `temperature = 1` yields a more unstable, but usually also more creative result. A balanced choice of e.g., `temperature = 0.7` is usually used.\n",
    "\n",
    "Try out how different temperature values affect the response generation for the same prompt:"
   ],
   "id": "5d6517674b293db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for temp in [0, 0.7, 1.0]:\n",
    "    pass"
   ],
   "id": "4e7ecc9059c7c61c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat Format: System, User, and Assistant Messages\n",
    "\n",
    "Instruction-tuned LLMs generally use different message roles, which we can leverage via the API:\n",
    "\n",
    "- **system**: sets behavior instructions, is usually passed as the first message in a chat to \"set the tone\"\n",
    "- **user**: represents human input messages, the prompt as you would enter it in an LLM\n",
    "- **assistant**: represents AI responses, the generated text received by the LLM (usually from an earlier conversation turn)\n",
    "\n",
    "Try out different system prompts where you command the model to take on different personas to see how its reponse to the actual prompt differs."
   ],
   "id": "c4dcf60b3376c9e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b75b34442a2c6126",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-turn Conversations\n",
    "\n",
    "LLMs can maintain context across multiple messages. Via the API, you can simulate conversations by appending responses and your own follow-up message to the message list. Generated responses are inserted using the `assistant` role, and the follow-up user message is then posed with the `user` role.\n",
    "\n",
    "Ask the LLM a question, append its response, and then ask a follow-up question to see how it maintains context across the whole conversation.\n"
   ],
   "id": "31d63c3414889e3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c04ec8d5b1723e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running Local LLMs\n",
    "\n",
    "Besides calling models via an API, you might want to run models locally, for example if you lack internet access, for experimentation without worrying about rate limits or API cost, or when working with privacy-sensitive data. In the following, we will use models loaded from the [Huggingface]() model repository and use them with the `transformers` library. As a rule of thumb, model below 400M parameters usually fit in 16GB RAM, with acceptable inference times on CPU."
   ],
   "id": "6811982572faac7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll use a small model that can run on a CPU: [`HuggingFaceTB/SmolLM2-360M-Instruct`](). We need both the model itself, and a tokenizer to convert the message blocks into token IDs the model can consume.",
   "id": "93c0952697e0f3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "device = \"cpu\" # \"gpu\" for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ],
   "id": "ca88165c4ea68e8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the tokenizer to turn the message list into usable model inputs. The [`tokenizer.apply_chat_template`]() function can directly produce the desired result. Important: specify the arguments `tokenize=True` and `return_tensors=\"pt\"` to get the correct Torch tensor objects for the model. Also remember to send the tokenized data to the same device as the model using the `.to(device)` method as already with the model.",
   "id": "42e285ee55be4340"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f6930572a5191b42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you can call the `model.generate()` function with the tokenized inputs to produce generated text. However, the model returns token IDs, not their string representation. You can convert the model output back into human-readable format using the `tokenizer.decode()` function.",
   "id": "92d40ee58b8bc867"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "799977a168404cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "To get the most out of any LLM, you need to carefully design prompts. Many prompting techniques exist and are hot topic in current research. The guides linked below provide a comprehensive overview on prompt engineering. In this notebook, we will explore three prompt engineering techniques:\n",
    "\n",
    "- Prompting-Induced Planning / Chain-of-Thought\n",
    "- Self-critique\n",
    "- Structured output prompting\n",
    "\n",
    "The following guides explore prompt engineering techniques in more detail:\n",
    "\n",
    "- [ðŸ”— Prompt Engineering Guide](https://drive.google.com/file/d/1AbaBYbEa_EbPelsT40-vj64L-2IwUJHy/view)\n",
    "- [ðŸ”— GPT4.1 Prompting Guide](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)\n"
   ],
   "id": "7a2523ef3c9a0806"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For prompt engineering, we will consider the RAG usecase with the example query and retrieved snippets below. The goal is to provide a short but informative answer to the user.",
   "id": "96da5397faf5bf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"why has olive oil increased in price\"\n",
    "snippets = [\n",
    "    (\"Global olive oil prices have surged due to poor harvests in Spain and Italy, caused by extreme drought and heatwaves linked to climate change.\", 0.983),\n",
    "    (\"A sharp decline in olive oil production, especially in major producing countries like Spain, has led to reduced supply and increased prices worldwide.\", 0.942),\n",
    "    (\"Increased production costs, including higher labor and transportation expenses, have contributed to the rise in olive oil prices.\", 0.891),\n",
    "    (\"Rising inflation and currency fluctuations have made imported goods, including olive oil, more expensive in several countries.\", 0.847),\n",
    "    (\"Retailers report that consumer demand for premium oils has increased, indirectly pushing prices higher across all olive oil grades.\", 0.812),\n",
    "    (\"Climate change has disrupted agricultural cycles in the Mediterranean region, impacting many crops including olives.\", 0.768),\n",
    "    (\"Sunflower oil shortages due to the war in Ukraine have led to a shift in demand toward olive oil, tightening global supply.\", 0.703),\n",
    "    (\"The Mediterranean diet, which emphasizes olive oil consumption, continues to gain popularity for its health benefits.\", 0.623),\n",
    "    (\"Spain is one of the world's largest producers of olive oil, exporting millions of liters each year.\", 0.578),\n",
    "    (\"Olive trees can live for hundreds of years and are cultivated mostly in Mediterranean climates.\", 0.519)\n",
    "]"
   ],
   "id": "457c33befff9a2f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vanilla RAG\n",
    "\n",
    "The most basic RAG prompting technique directly feeds retrieved context into the model alongside the query. This approach serves as a baseline: it simply instructs the model to answer the question based on the given context, without extra guidance.\n",
    "\n",
    "Implement the vanilla RAG prompt using a simple format with a system message and a user message containing the context and question."
   ],
   "id": "9cf4ddac0541163"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a2c596b01714aff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Chain-of-thought prompting encourages the model to reason step by step before producing a final answer. This helps improve factual accuracy and clarity, especially when the retrieved context is complex or contains multiple causal links.\n",
    "\n",
    "Implement CoT prompting by asking the model to first identify relevant information, then reason through the answer, and finally summarize the conclusion."
   ],
   "id": "91862c6c096de2fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ce596fd4c68c4f79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Critique-and-Revise Prompting\n",
    "\n",
    "To improve factuality or clarity, we can instruct the model to critique an initial answer before revising it. This multi-step prompting encourages reflection and refinement, and can lead to higher-quality outputs.\n",
    "\n",
    "Generate an initial answer, prompt the model to critique it, and then ask for a revised version based on that critique."
   ],
   "id": "d9495d633540af3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9a7ef76a26c1d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Self-Consistency\n",
    "\n",
    "As we have seen before, a model might produce different outputs at repeated inference when using higher temperatures. We can use this to our advantage by prompting the model for self-consistency: first, we generate several candidate answers at high temperature (leveraging the more creative, diverse output), and then provide these to the model to distill into a final answer at low temperature (yielding consistent output).\n",
    "\n",
    "Implement self-consistency by generating 3 candidates at high temperature, and combining them in a follow-up inference pass. Design special prompts for both phases."
   ],
   "id": "9819b471157ffdf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1080f7da91133d44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "62f1eb38b4ea1072",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Structured Output\n",
    "\n",
    "For downstream applications, you should have the model return structured outputs, e.g., in JSON dictionaries. Then, you can elicit responses that decompose their answer into different aspects. For example, you could refine the critique-and-revise technique from before my having the model return the critique and the final response as fields in a JSON dictionary, or separate reasoning and response in the answer in order to only display the response.\n",
    "\n",
    "For RAG, you can also use structured outputs to have the model attribute its reasoning to passages. Try to make it return its answer in a list of sentences, where for each sentence is represented as JSON dictionary with the text and the relevant passages information is taken from."
   ],
   "id": "6bbc16ee3c1ce782"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dd520cf1a42cb7dc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
