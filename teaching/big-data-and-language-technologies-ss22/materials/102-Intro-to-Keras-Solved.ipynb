{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8952fa6-1c50-43cd-97d1-a9a11efa837e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning in Python \n",
    "## Session 01 - Introduction to Keras\n",
    "\n",
    "- *Course*: Big Data and Language Technologies\n",
    "- *Date*: 04.04.2022\n",
    "\n",
    "In this session, we'll start with one of the most basic tasks in Natural Language Processing: training a model on text data to assign a numerical value between 0 and 1 corresponding to a single target feature. In this first session, we are going to use the `IMDB Movie Review Dataset` to perform sentiment classification, where 0 is negative sentiment, and 1 is positive sentiment.\n",
    "\n",
    "\n",
    "###### Errata\n",
    "- 2022/04/11: changed the tokenization to be fitted on training samples only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58872f-cbc1-42c5-899d-8ddc8b4d6dcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preface\n",
    "\n",
    "This notebook...\n",
    "\n",
    "... may feel basic if you've done statistical modeling or machine learning before. Don't worry, we will progress to more in-depth topics – these first sessions are thought to get everyone, regardless of their background and prior experience on the same level.\n",
    "\n",
    "... is divided into three steps:\n",
    "1. Load and preprocess the IMDB dataset\n",
    "2. Build a basic TensorFlow model using the Keras API\n",
    "3. Evaluate and select the best model and perform inference on unseen data\n",
    "\n",
    "... assumes three things:\n",
    "1. You have successfully completed the `101-Setup.md` guide and thus have a working environment with all dependencies installed\n",
    "2. You understand the basics of neural machine learning, with concepts like neurons, activation functions, loss functions, and optimization strategies\n",
    "3. You work in a UNIX-based environment (Linux, MacOS); if you're on Windows, you might need to adapt shell commands to your specific case\n",
    "\n",
    "\n",
    "... is organized as follows: each block of code cells is prefaced with a text cell containing a general explanation, then an **Exercise** (describing what is to be achieved in the following code cells), and optionally *Notes* (providing hints for the solution). Code cells might already contain some comments to help you along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30be34-5174-48b0-9ede-2ed49f250c4a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad06a31c-b4a7-49e3-a735-ae187b92cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core packages\n",
    "import pandas as pd     # High-level data wrangling\n",
    "import numpy as np      # Low-level data wrangling\n",
    "import tensorflow as tf # All the neural stuff\n",
    "\n",
    "# Helper functions\n",
    "from sklearn.model_selection import train_test_split # Splits a dataset randomly into train and test subsets\n",
    "from glob import glob                                # Expands a wildcard paths to all matching files for traversal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf66fed-d797-4119-acd3-ee9022f18e9e",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401d3bc-eff8-4ac0-a34b-7c2901a91bfe",
   "metadata": {},
   "source": [
    "The data can be downloaded by running the following cell. This will retrieve the archive and decompress it in your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d9c708-cb03-4df8-b3a8-a88d289c6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f087d-6462-4d35-ab1c-ecb8efa958df",
   "metadata": {},
   "source": [
    "We now need to convert the dataset into a form that we can explore and train models on.\n",
    "\n",
    "**Exercise**: load all the data into a `pandas.DataFrame` with the four columns `[id, text, label, set]` and one document per row.\n",
    "\n",
    "- `id` is the documents' ID\n",
    "- `text` is the documents' text\n",
    "- `label` is the documents labelled sentiment: `neg` or `pos`\n",
    "- `set` is the document set the document stems from: `train` or `test`\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c0d05e-4c1d-4c97-ac1d-04946eca00e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "PATH = \"./aclImdb/\"\n",
    "SETS = [\"train\", \"test\"]\n",
    "LABELS = [\"neg\", \"pos\"]\n",
    "\n",
    "for doc_set in SETS:\n",
    "    for label in LABELS:\n",
    "        for file in glob(PATH+doc_set+\"/\"+label+\"/*.txt\"):\n",
    "            file_id = file.split(\"/\")[-1].split(\".\")[0]\n",
    "            with open(file, \"r\") as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            doc = (file_id, text, label, doc_set)\n",
    "            df.append(doc)\n",
    "\n",
    "df = pd.DataFrame(df, columns=[\"id\", \"text\", \"label\", \"set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367c222e-bfd5-47ef-95b2-4668d513da52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1821_4</td>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>neg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10402_1</td>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1062_4</td>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>neg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9056_1</td>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5392_3</td>\n",
       "      <td>\"Carriers\" follows the exploits of two guys an...</td>\n",
       "      <td>neg</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text label    set\n",
       "0   1821_4  Working with one of the best Shakespeare sourc...   neg  train\n",
       "1  10402_1  Well...tremors I, the original started off in ...   neg  train\n",
       "2   1062_4  Ouch! This one was a bit painful to sit throug...   neg  train\n",
       "3   9056_1  I've seen some crappy movies in my life, but t...   neg  train\n",
       "4   5392_3  \"Carriers\" follows the exploits of two guys an...   neg  train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d42b42-44e5-4cd2-92d4-4184a97a5d08",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cc1d8-1d1d-491e-829b-1b6e5dd049ee",
   "metadata": {},
   "source": [
    "The next step is to preprocess the movie reviews. While for humans, the text and label as seen in the last step are interpretable, we cannot train a model directly on the text data: we need to convert texts and labels it into a numerical input and output space first.\n",
    "\n",
    "Since `train` and `test` set are independently used, we perform this individually for both document sets.\n",
    "\n",
    "**Exercise**: transform the text and label data into a suitable numeric representation. Encode the `pos` label as `1`, and the `neg` label as `0`. Convert all documents into sequences of term indices. The output should be the sequence sets `X_train` and `X_test`, and the label arrays `y_train` and `y_test`.\n",
    "\n",
    "*Notes*\n",
    "- Use the `tf.keras.preprocessing.text.Tokenizer` class to construct the document-term-matrix\n",
    "- Think about space savings in data types! An `int8` ist 8 times smaller than the default `int64` data type. And since our data does not exceed the number space representable by `int8`, we can fit the same data into less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a604cbc-91e4-4bac-bdc2-1d8258511355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the tokenizer on training set texts\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df.loc[df[\"set\"] == \"train\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0b71f9-3f26-4a43-9788-a1b418277278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training set documents into sequences\n",
    "X_train = tokenizer.texts_to_sequences(df.loc[df[\"set\"] == \"train\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13192c31-0c55-4630-876c-46712108bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test set documents into sequences\n",
    "X_test = tokenizer.texts_to_sequences(df.loc[df[\"set\"] == \"test\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799673a1-1e97-4590-9b8e-2de55d70561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training set labels into a 0-1 array\n",
    "y_train = df.loc[df[\"set\"] == \"train\", \"label\"].replace({\"neg\": 0, \"pos\": 1}).astype(np.int8).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df57d51-e89c-463f-984b-962eecf83400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test set labels into a 0-1 array\n",
    "y_test = df.loc[df[\"set\"] == \"test\", \"label\"].replace({\"neg\": 0, \"pos\": 1}).astype(np.int8).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d16e6-50ad-4f21-b250-2f19d58f8111",
   "metadata": {},
   "source": [
    "One problem remains: the model expects are input to be of equal length, yet the documents are varying. Therefore, they need to be padded with zeroes.\n",
    "\n",
    "**Exercise**: pad all sequences with zero to uniform length of `256`.\n",
    "\n",
    "*Notes*\n",
    "- use the `keras.preprocessing.sequence.pad_sequences` function\n",
    "- insert padding at the end of the sequences (*post-padding*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae17d91-6db3-449b-bf1b-60277cf94a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2eb1e-021f-4f8d-acf6-08af696c7edb",
   "metadata": {},
   "source": [
    "While we now have individual document sets to train and test on, we want to further split the training set into an actual training set and the validation set that is used to check the models performance after each training iteration.\n",
    "\n",
    "**Exercise** split the training data into an `X_train`/`y_train` and `X_val`/`Y_val` set with an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eefdebd-e73a-44a2-b452-694ef7ff65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea61a3-785c-4ee4-8b4b-e29c8473e7db",
   "metadata": {},
   "source": [
    "## Building a Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8da4a-2f9b-41f9-b429-1eb75f5b19b3",
   "metadata": {},
   "source": [
    "#### Sequential Network\n",
    "\n",
    "A `Sequential` model is a plain stack of layers, where each layer has exactly one input tensor and one output tensor. If we add multiple layers to the model, the output of the first will be passed as input to the second, the output of the second will be passed as input to the third, and so on. Note that input each layer has to have the same dimensionality as the previous output. \n",
    "\n",
    "The first and last layer are special: the first has to have an input dimensionality corresponding to the  training data. The last layer has to have the dimensionality of the desired output.\n",
    "\n",
    "**Exercise**: instatiate a `keras.Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b2a880-e776-4543-b30e-3a5fbc216882",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f543198-7483-4ce2-803a-a7d4db3984ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Embedding Layer\n",
    "A `Sequential` model behaves much like a list of layers: we can append to it using the `keras.Sequential.add` function, which takes the to-be-added layer as argument. First, we are going to add an input `Embedding` layer to the model.\n",
    "\n",
    "An `Embedding` layer turns positive integers (the term index in our document sequences) into dense vectors of fixed size. For example, we can turn a sequence of two indices into a two-dimensional vector of size two each: \n",
    "\n",
    "$$ \\begin{bmatrix}4 & 20\\end{bmatrix} \\rightarrow \\begin{bmatrix}\\begin{bmatrix}0.25\\\\0.1 \\\\\\end{bmatrix} \\begin{bmatrix}0.6\\\\-0.2 \\\\\\end{bmatrix}\\end{bmatrix} $$\n",
    "\n",
    "So, why do we need it? There are two reasons our `Sequential` model begins with an `Embedding` layer. \n",
    "\n",
    "The first is that we want to build a semantic representation of words, i.e. a vector space in which the distance between words relates to their semantic closeness. This is what the `Embedding` layer does: it transforms the discrete (word index) input space into a dense continuous vector space.\n",
    "\n",
    "The second reason is efficiency. The model cannot operate term indices directly, but instead needs a vectorized input. One option would be to use one-hot encoding to represent each index as a vector. For example, the index `44` in a vocabulary of 500 terms would be turned into a vector of 500 zeroes where the 44th place is a one. This leaves us with a vector representation that is both of extremely high dimensionality and extremely sparse - both are inefficient. The `Embedding` layer increases the efficiency by using much smaller, dense vectors. The index `44` can now be represented as a 16-dimension vector with continous, dense values. \n",
    "\n",
    "Taking both together, the `Embedding` layer increases both the efficiency and effectiveness of our network.\n",
    "\n",
    "**Exercise**: add an `keras.layers.Embedding` layer with 16 latent dimensions to the model. \n",
    "\n",
    "*Notes*\n",
    "- you can get the vocabulary size (i.e. input dimensionality) from the tokenizer. \n",
    "- remember to add 1 to the vocabulary size, since we have padded our sequence data  with zeroes, which created an additional \"empty\" term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0a1eb7-f618-4e1b-9c22-7aa0aaf8b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30766e5d-0a1e-4ad1-aa03-ea6cce6d6d10",
   "metadata": {},
   "source": [
    "#### Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06e9a8-2d65-4bb2-9133-8d4c4691d120",
   "metadata": {},
   "source": [
    "The previous `Embedding` layer produces an output tensor of size `(batch_size, 256, 16)` - the 256 is the sequence length we padded our documents to, and we have 16 latent dimensions in our term embeddings. Do not worry about the `batch_size` for now, it will be explained later.\n",
    "\n",
    "We cannot feed this tensor directly into a `Dense` layer: we still have a 3D tensor, but a `Dense` layer only accepts a 2D tensore as input. Enter: the `Pooling` layer. It performs a pooling operation to collapse one dimension of the tensor. In our case, we are going to use a `GlobalAveragePooling1D` layer: it calculates the average over one dimension of the input, collapsing it to a single number. You can think about this are reducing all the individual term embeddings into a single aggregated sequence embedding. The output dimensionality is therefore `(batch_size, 16)`.\n",
    "\n",
    "**Exercise**: add a `keras.layers.GlobalAveragePooling1D` layer to the  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36e723ee-c1c8-46cc-8f0b-5a270cfa252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.GlobalAveragePooling1D())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e9a7e-0885-4759-8252-61adeba50d3e",
   "metadata": {},
   "source": [
    "#### Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2564f-fff2-4d55-868b-5532d70f50fa",
   "metadata": {},
   "source": [
    "The next layer is a `Dense` layer: just your regular densely-connected neural layer, where $n$ inputs (in our case 16) are fully connected to $m$ output neurons (in our case 16, too). The `Dense` layer needs an activation function (ref. Lecture). We are going to use a fairy simple one: a `ReLU` (Rectifying Linear Unit).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/320px-Activation_rectified_linear.svg.png)\n",
    "\n",
    "It is just a linear activation of the input, that cannot fall below 0.\n",
    "\n",
    "**Exercise**: add a `keras.layers.Dense` layer with 16 dimensions and a ReLU activation function to the model.\n",
    "\n",
    "*Notes*:\n",
    "- the ReLU function is implemented as `tf.nn.relu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3802d09b-4b59-4c87-b9d0-1b0dc67ef6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502ac30-6f2c-4a98-9cca-8c5c0e85a6da",
   "metadata": {},
   "source": [
    "#### Dropout Layer\n",
    "To increase the robustness of our model, the layer is a `Dropout` layer. The `Dropout` layer randomly sets input units to 0 at each step during training time. This is also referred to as *regularization* (similar to regularization in linear regression problems) and helps prevent overfitting.\n",
    "\n",
    "**Exercise**: add a `keras.layers.Dropout` layer with a dropout rate of 0.1 to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b99e4903-affc-40eb-90dd-f008c2a4d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dropout(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc949dc-d2e9-42fd-b8c7-45f783543906",
   "metadata": {},
   "source": [
    "#### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99e205-6e22-49cd-a002-f2ac46940bca",
   "metadata": {},
   "source": [
    "The last layer of the model reduces the dimension of the output to the desired size. From the previous `Dropout` layer, we still have 16-dimensional tensors, but we want to have a single value between 0 and 1 to express the sentiment of the input. \n",
    "\n",
    "This is achieved by adding another `Dense` layer, but this time with an output dimensionality of `1`. Also, our activation function changes: we are going to use a `Sigmoid` activation.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n",
    "\n",
    "**Exercise**: add a `keras.layers.Dense` layer with 1 dimension and a sigmoid activation function to the model.\n",
    "\n",
    "*Notes*:\n",
    "- the sigmoid activation function is implemented as `tf.nn.sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72464128-389c-4642-875c-640898d7bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a0b4a-f78c-49c8-8862-341c95cf86e6",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "\n",
    "And thats it. We now have build the complete neural model to conduct sentiment classification. Before we continue, we will have a look at the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "049ce648-8f44-4309-b10f-613a3d793162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          1417328   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,417,617\n",
      "Trainable params: 1,417,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416a50e-0314-4f4b-9d15-6c2660703d2d",
   "metadata": {},
   "source": [
    "Note the signal flow of tensors within out model: the 3D output of the `Embedding` layer is reduced to 2D by the `Pooling` layer. This is kept by the following layers, and reduced to a single value in the last `Dense` layers. Also note the parameter counts for each layer: since `Pooling` and `Dropout` are just static operations on the tensors where nothing is learned, they dont have trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a29a84-afd5-48c1-a8b1-d03b8bba4a71",
   "metadata": {},
   "source": [
    "#### Compiling the model, Optimizer and Loss function\n",
    "\n",
    "In order to train the model, we first have to compile it. To understand the concepts introduced in this step, recall the well-known [mountain-descent metaphor](https://en.wikipedia.org/wiki/Gradient_descent#An_analogy_for_understanding_gradient_descent) for model training: you're on top of a mountain and try to get down (find the local minimum). But since there is heavy fog, you can only see your immediate surroundings. The gradient descent strategy would be to see where in your visible surroundings the descent down the mountain is steepest and move there.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Gradient_ascent_%28surface%29.png/585px-Gradient_ascent_%28surface%29.png)\n",
    "\n",
    "Compilation is the final step in defining a model. It specifies three things that influence the training procedure:\n",
    "\n",
    "1. A **Loss** function: the loss function is used to quantify the errors made during the learning process. Simply put, the lower the output value of the loss function, the better performing the model is; the training process therefore aims to *minimize* the loss. In the mountain-descent example, the loss indicates your current elevation.\n",
    "\n",
    "1. An **Optimizer**: the optimizer strategy is used to calculate the direction and magnitude of parameter updates during gradient descent. In the mountain-descent example, the optimizer is how you detect where to move next.\n",
    "\n",
    "\n",
    "3. A validation **Metric**: a metric is used to quantify the quality of the models predictions. While the loss quantifies the goodness-of-fit of the model parameters, the metric is calculated only on the output predictions of the model.\n",
    "\n",
    "\n",
    "**Exercise**: compile the model using the `adam` optimizer, the `binary_crossentropy` loss function, and the `accuracy` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c434a9b6-af96-41c7-8169-e6d4f208c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cada17-4b28-43e6-92ac-765b6800c2c2",
   "metadata": {},
   "source": [
    "#### Callbacks\n",
    "\n",
    "A callback is an action that is performed at a specific stage of training (e.g. at the start or end of an epoch, before or after a single batch, ...).\n",
    "\n",
    "We are going to use callbacks to:\n",
    "\n",
    "- save a checkpoint after each epoch if the validation loss improved\n",
    "- abort training if no validation loss improvement is apparent for 10 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b111f0-e3e2-4b92-afbf-ab955dd9d291",
   "metadata": {},
   "source": [
    "**Exercise**: instantiate a `keras.callbacks.ModelCheckpoint` that saves the best model according to the validation loss.\n",
    "\n",
    "*Notes*:\n",
    "- Keras saved models are usually named with a `*.h5` suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82a670ec-cccd-4056-92b3-7e229152718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"imdbsent.h5\",      # Path to save the model file\n",
    "    monitor=\"val_loss\", # The metric name to monitor\n",
    "    save_best_only=True # If True, it only saves the \"best\" model according to the quantity monitored \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c77724e-05c5-4587-8cb2-b6e5e5e37e8b",
   "metadata": {},
   "source": [
    "**Exercise**: instantiate a `keras.callbacks.EarlyStopping` that aborts training when no improvement in validation loss has taken place for 10 iterations.\n",
    "\n",
    "*Notes*:\n",
    "- Use a delta of $\\pm 0.01$ to indicate improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f5cfb56-9045-4e79-a254-e4bd31878c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", # Quantity to be monitored.\n",
    "    min_delta=0.01,     # Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "    patience=10,        # Number of epochs with no improvement after which training will be stopped.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b342e4-dd3a-426c-be12-e0c4e3fd590c",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009cdca-cfb8-4881-8fdf-3caac97c3dd3",
   "metadata": {},
   "source": [
    "At this point, we assembled all components needed to train our first model:\n",
    "- preprocessed training, testing, and validation datas\n",
    "- model architecture\n",
    "- callbacks\n",
    "\n",
    "**Exercise**: fit the model on the training data for 100 epochs with a batch size of 512, evaluation on the validation subset. Use both previously instantiated callback functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50c2b123-4de4-4299-9897-342c8e00f4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/40 [..............................] - ETA: 6s - loss: 0.6931 - acc: 0.4883"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 13:54:33.552966: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 11ms/step - loss: 0.6913 - acc: 0.6041 - val_loss: 0.6876 - val_acc: 0.7304\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6809 - acc: 0.7286 - val_loss: 0.6718 - val_acc: 0.7934\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6568 - acc: 0.7830 - val_loss: 0.6399 - val_acc: 0.8008\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.6138 - acc: 0.8002 - val_loss: 0.5912 - val_acc: 0.8172\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.5561 - acc: 0.8240 - val_loss: 0.5340 - val_acc: 0.8324\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.4941 - acc: 0.8468 - val_loss: 0.4771 - val_acc: 0.8472\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.4339 - acc: 0.8674 - val_loss: 0.4266 - val_acc: 0.8612\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.3800 - acc: 0.8850 - val_loss: 0.3853 - val_acc: 0.8716\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.3366 - acc: 0.8961 - val_loss: 0.3521 - val_acc: 0.8770\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.2971 - acc: 0.9079 - val_loss: 0.3273 - val_acc: 0.8820\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.2655 - acc: 0.9158 - val_loss: 0.3085 - val_acc: 0.8860\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.2405 - acc: 0.9258 - val_loss: 0.2939 - val_acc: 0.8884\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.2189 - acc: 0.9317 - val_loss: 0.2833 - val_acc: 0.8910\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1992 - acc: 0.9380 - val_loss: 0.2749 - val_acc: 0.8932\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1819 - acc: 0.9451 - val_loss: 0.2688 - val_acc: 0.8946\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1673 - acc: 0.9506 - val_loss: 0.2633 - val_acc: 0.8970\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1541 - acc: 0.9536 - val_loss: 0.2609 - val_acc: 0.8974\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1430 - acc: 0.9578 - val_loss: 0.2571 - val_acc: 0.8986\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1317 - acc: 0.9610 - val_loss: 0.2556 - val_acc: 0.8998\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1223 - acc: 0.9647 - val_loss: 0.2536 - val_acc: 0.9010\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1134 - acc: 0.9690 - val_loss: 0.2526 - val_acc: 0.9014\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1042 - acc: 0.9730 - val_loss: 0.2529 - val_acc: 0.9002\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.0975 - acc: 0.9747 - val_loss: 0.2547 - val_acc: 0.8998\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0907 - acc: 0.9778 - val_loss: 0.2535 - val_acc: 0.9012\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.0839 - acc: 0.9787 - val_loss: 0.2544 - val_acc: 0.9002\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0785 - acc: 0.9809 - val_loss: 0.2559 - val_acc: 0.8996\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0730 - acc: 0.9833 - val_loss: 0.2584 - val_acc: 0.9002\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0678 - acc: 0.9844 - val_loss: 0.2602 - val_acc: 0.8998\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1, # print result every epoch\n",
    "    batch_size=512,\n",
    "    callbacks = [checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83580226-04a2-47ab-a0d6-80bcaea7f8e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loading a Model\n",
    "\n",
    "During training, the `ModelCheckpoint` callback continously persisted the best performing model to disk. Since the last epoch was not necessarily the best one, before we proceed with evaluation and inference, we can now load the best checkpoint to get the optimal model version.\n",
    "\n",
    "**Exercise**: overwrite the `model` variable with the best performing version by loading the latest checkpoint from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9413da33-e502-4c8d-b7f9-01500bdd096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"imdbsent.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d585a33-ed89-4388-9241-24d8b6f0757d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d6a3e-b6c4-47b7-ab82-c0e548b3b5b7",
   "metadata": {},
   "source": [
    "To quantify the true performance of the model, we are going to infer scores for the test set and compare them to the true gold labels, using `accuracy` as a metric. Since the model has not seen this data before, this score is expected to be lower than the valudation accuracy, yet allows us to gain insight into how well the model generalizes beyond its training data.\n",
    "\n",
    "**Exercise** use the model evaluation function to calculate the test loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5470f6b-e53c-47d5-b345-8d45c52972de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3\n",
      "Test accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0) \n",
    "\n",
    "print('Test loss:', round(score[0], 2)) \n",
    "print('Test accuracy:', round(score[1], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31d1f6-57d5-4e8d-a17f-2cd7ebd2b49f",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Finally, with the model trained and evaluated to ensure it produces reliable(ish) results, the only missing piece is *prediction*: inferring a sentiment judgement for any arbitrary input of text.\n",
    "\n",
    "**Exercise**: write a `predict(queries: np.array) -> np.array` function that performs sentiment inference for a given unknown string. It should output sentiment scores in the range of [-1...1] (negative to positive)\n",
    "\n",
    "*Notes*:\n",
    "- the input string has to undergo the exact same preprocessing pipeline as the data the model was trained on (tokenization, padding, ...)\n",
    "- it is customary to construct inference functions to take a batch of potentially many inputs and returns a list of predicted labels, instead of just a single query\n",
    "- the model naturally outputs scores on the [0...1] range – you need to rescale that to the desired [-1...1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe74206-e637-4ed2-9601-ff4d9b1a43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(queries: np.array) -> np.array:\n",
    "    sequences = tokenizer.texts_to_sequences(queries)\n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post', maxlen=256)\n",
    "    return (model.predict(sequences) * 2) -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb5f44-df16-41e5-a78c-3cdde17ad175",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise**: perform predictions for some sample inputs and see how well your model works. Can you find a case of clear misassignment, i.e. the model predicting wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2d24f2d-524b-4aeb-9a4b-c5de55cc8422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06828535],\n",
       "       [ 0.26398706],\n",
       "       [-0.28701127]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([\"This is a good test sentence.\", \"This is perfect test sentence.\", \"This a a bad test sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98d690b5-33b0-48c8-bfdb-37e4579ba548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3305167]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([\"This is not a bad example at all.\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
